{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciSketch - The AI Powered BioRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (4.66.2)\n",
      "Requirement already satisfied: dask in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2024.5.1)\n",
      "Requirement already satisfied: cairosvg in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2.7.1)\n",
      "Requirement already satisfied: pillow in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (3.9.0)\n",
      "Requirement already satisfied: easyocr in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (1.5.0)\n",
      "Requirement already satisfied: opencv-python-headless in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: pandas in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: distributed in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2024.5.1)\n",
      "Requirement already satisfied: torch in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: pyspark in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (3.5.1)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (58.0.4)\n",
      "Requirement already satisfied: click>=8.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (24.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from dask) (7.0.2)\n",
      "Requirement already satisfied: cairocffi in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cairosvg) (1.7.0)\n",
      "Requirement already satisfied: cssselect2 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cairosvg) (0.7.0)\n",
      "Requirement already satisfied: defusedxml in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cairosvg) (0.7.1)\n",
      "Requirement already satisfied: tinycss2 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cairosvg) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: scipy in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (1.13.0)\n",
      "Requirement already satisfied: scikit-image in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (0.22.0)\n",
      "Requirement already satisfied: python-bidi in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (0.4.2)\n",
      "Requirement already satisfied: Shapely in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (2.0.4)\n",
      "Requirement already satisfied: pyclipper in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (1.3.0.post5)\n",
      "Requirement already satisfied: ninja in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from easyocr) (1.11.1.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (3.1.3)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (1.0.8)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (5.9.8)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (6.4)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (2.2.1)\n",
      "Requirement already satisfied: zict>=3.0.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.13.0->dask) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from jinja2>=2.10.3->distributed) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.1.0 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cairocffi->cairosvg) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cssselect2->cairosvg) (0.5.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from scikit-image->easyocr) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from scikit-image->easyocr) (2024.5.10)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /Users/stevensu/Library/Python/3.9/lib/python/site-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm dask cairosvg pillow matplotlib easyocr scikit-learn opencv-python-headless pandas distributed torch torchvision torchaudio pyspark setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>saved_image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tumor immune surveillance and cancer immunothe...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Dataset/Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MicroRNAs (miRNAs) regulate gene expression th...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Dataset/Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The mechanisms by which ubiquitin ligases are ...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Dataset/Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The maintenance of H3K9 and DNA methylation at...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Dataset/Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We report that diffuse large B cell lymphoma (...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Dataset/Imag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  Tumor immune surveillance and cancer immunothe...   \n",
       "1  MicroRNAs (miRNAs) regulate gene expression th...   \n",
       "2  The mechanisms by which ubiquitin ligases are ...   \n",
       "3  The maintenance of H3K9 and DNA methylation at...   \n",
       "4  We report that diffuse large B cell lymphoma (...   \n",
       "\n",
       "                                    saved_image_path  \n",
       "0  /Users/stevensu/Desktop/SciSketch-Dataset/Imag...  \n",
       "1  /Users/stevensu/Desktop/SciSketch-Dataset/Imag...  \n",
       "2  /Users/stevensu/Desktop/SciSketch-Dataset/Imag...  \n",
       "3  /Users/stevensu/Desktop/SciSketch-Dataset/Imag...  \n",
       "4  /Users/stevensu/Desktop/SciSketch-Dataset/Imag...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Final-csv.csv')\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns=['Link', 'Link_to_img'], inplace=True)\n",
    "\n",
    "def process_image_path(image_path):\n",
    "    base_path = '/Users/stevensu/Desktop/SciSketch-Dataset'\n",
    "    # Find the position of '/Images'\n",
    "    position = image_path.find('/Images')\n",
    "    if position != -1:\n",
    "        return base_path + image_path[position:]\n",
    "    else:\n",
    "        return base_path + image_path  # Concatenate base_path even if '/Images' is not found\n",
    "\n",
    "df['saved_image_path'] = df['saved_image_path'].apply(process_image_path)\n",
    "\n",
    "# Print the processed dataframe\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:06:03.779 Python[23679:397779] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType\n",
    "import os\n",
    "\n",
    "# Define the function to process images\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Draw bounding boxes and texts on the image\n",
    "    for box, text in zip(boxes, texts):\n",
    "        # Draw the bounding box\n",
    "        top_left = tuple(map(int, box[0]))\n",
    "        bottom_right = tuple(map(int, box[2]))\n",
    "        cv2.rectangle(original_image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "        # Put the OCR text near the bounding box\n",
    "        cv2.putText(original_image, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=70, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "\n",
    "    # Print the image with bounding boxes\n",
    "    cv2.imshow('Annotated Image', original_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return text_annotations\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/stevensu/Desktop/SciSketch-Dataset/Images/1-s2.0-S2666979X2300085X-fx1_lrg.jpg'\n",
    "annotations = process_image(image_path)\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Crosslink RNA to protein', 'coordinates': [182, 32, 393, 109]}, {'text': '228', 'coordinates': [792, 38, 848, 70]}, {'text': 'eCLIPs', 'coordinates': [858, 37, 964, 69]}, {'text': 'CDS', 'coordinates': [688, 92, 740, 122]}, {'text': \"5' UTR\", 'coordinates': [580, 134, 670, 164]}, {'text': 'Immunoprecipitate RNA-protein complexes', 'coordinates': [182, 140, 521, 217]}, {'text': 'RNA', 'coordinates': [702, 174, 754, 202]}, {'text': 'Splice', 'coordinates': [798, 190, 892, 222]}, {'text': 'site', 'coordinates': [900, 190, 964, 218]}, {'text': 'Sequence and align reads', 'coordinates': [182, 244, 393, 323]}, {'text': 'tRNA/ snRNA', 'coordinates': [618, 264, 770, 294]}, {'text': 'SnoRNA', 'coordinates': [508, 280, 598, 306]}, {'text': 'MtRNA', 'coordinates': [732, 302, 814, 332]}, {'text': \"3' UTR\", 'coordinates': [876, 310, 966, 340]}, {'text': 'Call RNA elements enriched over input', 'coordinates': [183, 353, 459, 431]}, {'text': 'Intron', 'coordinates': [796, 356, 890, 382]}, {'text': 'Evaluate performance of binding site detection', 'coordinates': [174, 452, 824, 501]}, {'text': 'GUUCAAACUCAUGG GAGUGUGACCUAAU AAUGUUCAGGCUUU', 'coordinates': [604, 510, 814, 594]}, {'text': '=', 'coordinates': [884, 512, 972, 584]}, {'text': 'Discover constrained binding via in silico perturbation reference common rare very rare singleton', 'coordinates': [127, 635, 869, 809]}, {'text': 'binding binding', 'coordinates': [71, 689, 184, 790]}, {'text': 'AGTGCCTGTGGTTGTCAGA', 'coordinates': [638, 678, 922, 708]}, {'text': 'GT G C C TC C A A C A A CCAG G A', 'coordinates': [638, 706, 921, 808]}, {'text': 'Nominate new splicing regulators', 'coordinates': [266, 830, 735, 880]}]\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType\n",
    "import os\n",
    "\n",
    "# Define the function to process images\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=70, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "\n",
    "    return text_annotations\n",
    "\n",
    "# # Define the UDF to be used in PySpark\n",
    "# @udf(returnType=ArrayType(StructType([\n",
    "#     StructField(\"text\", StringType(), True),\n",
    "#     StructField(\"coordinates\", ArrayType(IntegerType()), True)\n",
    "# ])))\n",
    "\n",
    "# def process_image_udf(image_path):\n",
    "#     return process_image(image_path)\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"ImageProcessingApp\").getOrCreate()\n",
    "\n",
    "# # Define schema for DataFrame\n",
    "# schema = StructType([\n",
    "#     StructField(\"Abstract\", StringType(), True),\n",
    "#     StructField(\"saved_image_path\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # Convert the processed Pandas DataFrame to a Spark DataFrame\n",
    "# sdf = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "# # Repartition the DataFrame\n",
    "# sdf = sdf.repartition(40)  # Adjust the number based on your system's capacity\n",
    "\n",
    "# # Apply the UDF to process images\n",
    "# sdf = sdf.withColumn(\"text_annotations\", process_image_udf(sdf[\"saved_image_path\"]))\n",
    "\n",
    "# # Select the relevant columns\n",
    "# sdf = sdf.select(\"Abstract\", \"text_annotations\")\n",
    "\n",
    "# # Convert to JSON format and save the output\n",
    "# sdf.write.json(\"/Users/stevensu/Desktop/SciSketch-Dataset/processed_data.json\")\n",
    "\n",
    "# # Stop Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:167\u001b[0m, in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image_path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load the main image using OpenCV\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/__init__.py:157\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Set a global controller that can be used to locally limit the number of\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# threads without looping through all shared libraries every time.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# This instantitation should not happen earlier because it needs all BLAS and\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# OpenMP libs to be loaded first.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     _threadpool_controller \u001b[38;5;241m=\u001b[39m \u001b[43mThreadpoolController\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_module\u001b[39m(module):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fixture for the tests to assure globally controllable seeding of RNGs\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:818\u001b[0m, in \u001b[0;36mThreadpoolController.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib_controllers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:970\u001b[0m, in \u001b[0;36mThreadpoolController._load_libraries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded shared libraries and store the supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_libraries_with_dyld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_libraries_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:1040\u001b[0m, in \u001b[0;36mThreadpoolController._find_libraries_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1037\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# Store the library controller if it is supported and selected\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_controller_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:1134\u001b[0m, in \u001b[0;36mThreadpoolController._make_controller_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Store a library controller if it is supported and selected\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# Required to resolve symlinks\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[43m_realpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# `lower` required to take account of OpenMP dll case on Windows\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# (vcomp, VCOMP, Vcomp, ...)\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filepath)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:548\u001b[0m, in \u001b[0;36m_realpath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_realpath\u001b[39m(filepath):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Small caching wrapper around os.path.realpath to limit system calls\"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:391\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 391\u001b[0m     path, ok \u001b[38;5;241m=\u001b[39m \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:425\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, seen)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    424\u001b[0m newpath \u001b[38;5;241m=\u001b[39m join(path, name)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mislink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    426\u001b[0m     path \u001b[38;5;241m=\u001b[39m newpath\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:167\u001b[0m, in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a symbolic link\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=50, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "\n",
    "    return text_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 154/24683 [07:52<20:54:35,  3.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             training_data\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n\u001b[0;32m---> 32\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     35\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(training_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mcreate_training_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m training_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         training_data\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_image_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process the image and get text annotations\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m text_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Skip if the image could not be processed\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize the EasyOCR reader with GPU support if available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 20\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use MPS (Metal Performance Shaders) backend\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m], gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/easyocr/easyocr.py:231\u001b[0m, in \u001b[0;36mReader.__init__\u001b[0;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     network_params \u001b[38;5;241m=\u001b[39m recog_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m \u001b[43mget_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecog_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdict_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/easyocr/recognition.py:182\u001b[0m, in \u001b[0;36mget_recognizer\u001b[0;34m(recog_network, network_params, character, separator_list, dict_list, model_path, device, quantize)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(model)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 182\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, converter\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1313\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:307\u001b[0m, in \u001b[0;36m_mps_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mps_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/storage.py:144\u001b[0m, in \u001b[0;36m_StorageBase.mps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a MPS copy of this storage if it's not already on the MPS.\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# def process_row(row):\n",
    "#     abstract_text = row['Abstract']\n",
    "#     image_path = row['saved_image_path']\n",
    "\n",
    "#     # Process the image and get text annotations\n",
    "#     text_annotations = process_image(image_path)\n",
    "\n",
    "#     # Skip if the image could not be processed\n",
    "#     if text_annotations is None:\n",
    "#         return None\n",
    "\n",
    "#     # Create the structured data entry\n",
    "#     data_entry = {\n",
    "#         'abstract_text': abstract_text,\n",
    "#         'text_annotations': text_annotations\n",
    "#     }\n",
    "\n",
    "#     return data_entry\n",
    "\n",
    "# def create_training_data(df):\n",
    "#     training_data = []\n",
    "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "#         result = process_row(row)\n",
    "#         if result is not None:\n",
    "#             training_data.append(result)\n",
    "    \n",
    "#     return training_data\n",
    "\n",
    "# training_data = create_training_data(df)\n",
    "\n",
    "# with open('training_data.json', 'w') as f:\n",
    "#     json.dump(training_data, f, indent=4)\n",
    "\n",
    "# # Print the structured training data\n",
    "# for entry in training_data:\n",
    "#     print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_data_for_bert(training_data):\n",
    "    inputs = []\n",
    "    text_labels = []\n",
    "    coord_labels = []\n",
    "\n",
    "    for entry in training_data:\n",
    "        abstract_text = entry['abstract_text']\n",
    "        text_annotations = entry['text_annotations']\n",
    "\n",
    "        # Tokenize the abstract text\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            abstract_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Create labels for text presence and coordinates\n",
    "        text_label = torch.zeros(input_ids.size())\n",
    "        coord_label = torch.zeros((input_ids.size(0), 4))\n",
    "\n",
    "        for annotation in text_annotations:\n",
    "            text = annotation['text']\n",
    "            coordinates = annotation['coordinates']\n",
    "\n",
    "            # Find the start and end tokens of the text in the abstract\n",
    "            start_idx = abstract_text.find(text)\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(text) - 1\n",
    "\n",
    "                # Convert character indices to token indices\n",
    "                token_start_idx = tokenizer.encode(abstract_text[:start_idx], add_special_tokens=False)\n",
    "                token_end_idx = tokenizer.encode(abstract_text[:end_idx + 1], add_special_tokens=False)\n",
    "\n",
    "                token_start_idx = len(token_start_idx)\n",
    "                token_end_idx = len(token_end_idx)\n",
    "\n",
    "                text_label[token_start_idx:token_end_idx] = 1  # Mark the presence of text\n",
    "                coord_label[token_start_idx:token_end_idx, :] = torch.tensor(coordinates)  # Assign coordinates\n",
    "\n",
    "        inputs.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        })\n",
    "        text_labels.append(text_label)\n",
    "        coord_labels.append(coord_label)\n",
    "\n",
    "    return inputs, text_labels, coord_labels\n",
    "\n",
    "inputs, text_labels, coord_labels = prepare_data_for_bert(training_data)\n",
    "\n",
    "# Example of how the inputs and labels look\n",
    "print(inputs[0])\n",
    "print(text_labels[0])\n",
    "print(coord_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:   5%|▍         | 112/2437 [00:04<02:10, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/virus.svg: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  15%|█▌        | 377/2437 [00:18<00:53, 38.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_spiral.svg: <urlopen error [Errno 2] No such file or directory: '/Users/stevensu/Desktop/BioIcons All Images/7444463B.jpg'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  88%|████████▊ | 2141/2437 [01:41<00:09, 31.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Polyomaviridae.svg: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  90%|████████▉ | 2191/2437 [01:44<00:11, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_coccus.svg: <urlopen error [Errno 2] No such file or directory: '/Users/stevensu/Desktop/BioIcons All Images/DA34F080.jpg'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  97%|█████████▋| 2354/2437 [01:54<00:03, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Anterior_view_of_ligaments_of_pelvis.svg: junk after document element: line 3993, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG: 100%|██████████| 2437/2437 [01:58<00:00, 20.49it/s]\n",
      "Template Matching: 100%|██████████| 6381/6381 [00:00<00:00, 27773.16it/s]\n",
      "[ WARN:10@2670.354] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/virus_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:4@2672.677] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Anterior_view_of_ligaments_of_pelvis_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:4@2697.742] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Polyomaviridae_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:9@2720.353] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_coccus_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:6@2740.402] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_spiral_converted.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import cairosvg\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "# from dask import delayed, compute\n",
    "\n",
    "# def convert_svg_to_png(svg_path, output_path):\n",
    "#     try:\n",
    "#         cairosvg.svg2png(url=svg_path, write_to=output_path, unsafe=True)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error converting {svg_path}: {e}\")\n",
    "\n",
    "# # Load the main image using OpenCV\n",
    "# image_path = '/Users/stevensu/Desktop/SciSketch-Dataset/Images/1-s2.0-S2667290121000632-gr1_lrg.jpg'  # Update with the correct path if needed\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Check if the image was loaded successfully\n",
    "# if image is None:\n",
    "#     raise FileNotFoundError(f\"Cannot open image file: {image_path}\")\n",
    "\n",
    "# original_image = image.copy()\n",
    "\n",
    "# # List of template image paths\n",
    "# png_template_paths = glob.glob('/Users/stevensu/Desktop/Servier Medical Art All Images/*.png')\n",
    "# svg_template_paths = glob.glob('/Users/stevensu/Desktop/BioIcons All Images/*.svg')\n",
    "\n",
    "# # Convert SVG templates to PNG\n",
    "# for svg_path in tqdm(svg_template_paths, desc=\"Converting SVG to PNG\"):\n",
    "#     output_path = svg_path.replace('.svg', '_converted.png')\n",
    "#     convert_svg_to_png(svg_path, output_path)\n",
    "#     png_template_paths.append(output_path)\n",
    "\n",
    "# @delayed\n",
    "# def match_template(template_path):\n",
    "#     template = cv2.imread(template_path, 0)\n",
    "#     if template is None:\n",
    "#         return []\n",
    "\n",
    "#     if template.shape[0] > original_image.shape[0] or template.shape[1] > original_image.shape[1]:\n",
    "#         return []\n",
    "\n",
    "#     w, h = template.shape[::-1]\n",
    "#     res = cv2.matchTemplate(cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY), template, cv2.TM_CCOEFF_NORMED)\n",
    "#     threshold = 0.8  # Adjust this threshold as needed\n",
    "#     loc = np.where(res >= threshold)\n",
    "    \n",
    "#     boxes = []\n",
    "#     for pt in zip(*loc[::-1]):\n",
    "#         box = [pt, (pt[0] + w, pt[1] + h)]\n",
    "#         boxes.append(box)\n",
    "#         cv2.rectangle(image, pt, (pt[0] + w, pt[1] + h), (255, 0, 0), 2)\n",
    "    \n",
    "#     return boxes\n",
    "\n",
    "# # Template matching with progress bar\n",
    "# tasks = []\n",
    "# for template_path in tqdm(png_template_paths, desc=\"Template Matching\"):\n",
    "#     result = match_template(template_path)\n",
    "#     tasks.append(result)\n",
    "\n",
    "# # Compute the results in parallel\n",
    "# computed_results = compute(*tasks)\n",
    "\n",
    "# # Collect all detected icon locations\n",
    "# icon_locations = []\n",
    "# for loc in computed_results:\n",
    "#     if loc:\n",
    "#         icon_locations.extend(loc)\n",
    "\n",
    "# # Draw bounding boxes for detected icons\n",
    "# for loc in icon_locations:\n",
    "#     cv2.rectangle(image, loc[0], loc[1], (255, 0, 0), 2)\n",
    "\n",
    "# # Display the image with bounding boxes\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
