{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciSketch - The AI Powered BioRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting dask\n",
      "  Downloading dask-2024.5.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting cairosvg\n",
      "  Using cached CairoSVG-2.7.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting easyocr\n",
      "  Using cached easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting distributed\n",
      "  Downloading distributed-2024.5.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.18.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting click>=8.1 (from dask)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=1.5.0 (from dask)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from dask) (24.0)\n",
      "Collecting partd>=1.2.0 (from dask)\n",
      "  Using cached partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.3.1 (from dask)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting cairocffi (from cairosvg)\n",
      "  Using cached cairocffi-1.7.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting cssselect2 (from cairosvg)\n",
      "  Using cached cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting defusedxml (from cairosvg)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting tinycss2 (from cairosvg)\n",
      "  Using cached tinycss2-1.3.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m201.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting scipy (from easyocr)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m262.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image (from easyocr)\n",
      "  Using cached scikit_image-0.23.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Using cached python_bidi-0.4.2-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Using cached shapely-2.0.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Using cached pyclipper-1.3.0.post5-cp312-cp312-macosx_10_9_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (5.3 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting jinja2>=2.10.3 (from distributed)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting locket>=1.0.0 (from distributed)\n",
      "  Using cached locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting msgpack>=1.0.0 (from distributed)\n",
      "  Using cached msgpack-1.0.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from distributed) (5.9.8)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed)\n",
      "  Using cached tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from distributed) (6.4)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from distributed) (2.2.1)\n",
      "Collecting zict>=3.0.0 (from distributed)\n",
      "  Using cached zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.10.3->distributed)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stevensu/Desktop/SciSketch-Summer/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting cffi>=1.1.0 (from cairocffi->cairosvg)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting webencodings (from cssselect2->cairosvg)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr)\n",
      "  Using cached imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr)\n",
      "  Downloading tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pycparser (from cffi>=1.1.0->cairocffi->cairosvg)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading dask-2024.5.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m390.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached CairoSVG-2.7.1-py3-none-any.whl (43 kB)\n",
      "Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached matplotlib-3.9.0-cp312-cp312-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
      "Downloading scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached opencv_python_headless-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading distributed-2024.5.2-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached torchvision-0.18.0-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.3.0-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (245 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.0-cp312-cp312-macosx_11_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Using cached msgpack-1.0.8-cp312-cp312-macosx_11_0_arm64.whl (85 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Downloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Using cached cairocffi-1.7.0-py3-none-any.whl (75 kB)\n",
      "Using cached cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "Using cached pyclipper-1.3.0.post5-cp312-cp312-macosx_10_9_universal2.whl (278 kB)\n",
      "Using cached python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
      "Using cached scikit_image-0.23.2-cp312-cp312-macosx_12_0_arm64.whl (13.3 MB)\n",
      "Using cached shapely-2.0.4-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tinycss2-1.3.0-py3-none-any.whl (22 kB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (177 kB)\n",
      "Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.5/225.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: webencodings, sortedcontainers, pytz, pyclipper, py4j, ninja, mpmath, zict, tzdata, typing-extensions, tqdm, toolz, tinycss2, threadpoolctl, tblib, sympy, setuptools, pyyaml, python-bidi, pyspark, pyparsing, pycparser, pillow, numpy, networkx, msgpack, MarkupSafe, locket, lazy-loader, kiwisolver, joblib, fsspec, fonttools, filelock, defusedxml, cycler, cloudpickle, click, tifffile, Shapely, scipy, partd, pandas, opencv-python-headless, jinja2, imageio, cssselect2, contourpy, cffi, torch, scikit-learn, scikit-image, matplotlib, dask, cairocffi, torchvision, torchaudio, distributed, cairosvg, easyocr\n",
      "Successfully installed MarkupSafe-2.1.5 Shapely-2.0.4 cairocffi-1.7.0 cairosvg-2.7.1 cffi-1.16.0 click-8.1.7 cloudpickle-3.0.0 contourpy-1.2.1 cssselect2-0.7.0 cycler-0.12.1 dask-2024.5.2 defusedxml-0.7.1 distributed-2024.5.2 easyocr-1.7.1 filelock-3.14.0 fonttools-4.53.0 fsspec-2024.5.0 imageio-2.34.1 jinja2-3.1.4 joblib-1.4.2 kiwisolver-1.4.5 lazy-loader-0.4 locket-1.0.0 matplotlib-3.9.0 mpmath-1.3.0 msgpack-1.0.8 networkx-3.3 ninja-1.11.1.1 numpy-1.26.4 opencv-python-headless-4.9.0.80 pandas-2.2.2 partd-1.4.2 pillow-10.3.0 py4j-0.10.9.7 pyclipper-1.3.0.post5 pycparser-2.22 pyparsing-3.1.2 pyspark-3.5.1 python-bidi-0.4.2 pytz-2024.1 pyyaml-6.0.1 scikit-image-0.23.2 scikit-learn-1.5.0 scipy-1.13.1 setuptools-70.0.0 sortedcontainers-2.4.0 sympy-1.12.1 tblib-3.0.0 threadpoolctl-3.5.0 tifffile-2024.5.22 tinycss2-1.3.0 toolz-0.12.1 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 tqdm-4.66.4 typing-extensions-4.12.1 tzdata-2024.1 webencodings-0.5.1 zict-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm dask cairosvg pillow matplotlib easyocr scikit-learn opencv-python-headless pandas distributed torch torchvision torchaudio pyspark setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>saved_image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tumor immune surveillance and cancer immunothe...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MicroRNAs (miRNAs) regulate gene expression th...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The mechanisms by which ubiquitin ligases are ...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The maintenance of H3K9 and DNA methylation at...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We report that diffuse large B cell lymphoma (...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  Tumor immune surveillance and cancer immunothe...   \n",
       "1  MicroRNAs (miRNAs) regulate gene expression th...   \n",
       "2  The mechanisms by which ubiquitin ligases are ...   \n",
       "3  The maintenance of H3K9 and DNA methylation at...   \n",
       "4  We report that diffuse large B cell lymphoma (...   \n",
       "\n",
       "                                    saved_image_path  \n",
       "0  /Users/stevensu/Desktop/SciSketch-Summer/Image...  \n",
       "1  /Users/stevensu/Desktop/SciSketch-Summer/Image...  \n",
       "2  /Users/stevensu/Desktop/SciSketch-Summer/Image...  \n",
       "3  /Users/stevensu/Desktop/SciSketch-Summer/Image...  \n",
       "4  /Users/stevensu/Desktop/SciSketch-Summer/Image...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Final-csv.csv')\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns=['Link', 'Link_to_img'], inplace=True)\n",
    "\n",
    "def process_image_path(image_path):\n",
    "    base_path = '/Users/stevensu/Desktop/SciSketch-Summer'\n",
    "    # Find the position of '/Images'\n",
    "    position = image_path.find('/Images')\n",
    "    if position != -1:\n",
    "        return base_path + image_path[position:]\n",
    "    else:\n",
    "        return base_path + image_path  # Concatenate base_path even if '/Images' is not found\n",
    "\n",
    "df['saved_image_path'] = df['saved_image_path'].apply(process_image_path)\n",
    "\n",
    "# Print the processed dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType\n",
    "import os\n",
    "\n",
    "# Define the function to process images\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Draw bounding boxes and texts on the image\n",
    "    for box, text in zip(boxes, texts):\n",
    "        # Draw the bounding box\n",
    "        top_left = tuple(map(int, box[0]))\n",
    "        bottom_right = tuple(map(int, box[2]))\n",
    "        cv2.rectangle(original_image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "        # Put the OCR text near the bounding box\n",
    "        cv2.putText(original_image, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=70, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,w\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "        \n",
    "            # Display the image with bounding boxes and OCR results\n",
    "    cv2.imshow('Processed Image', original_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return text_annotations\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/stevensu/Desktop/SciSketch-Summer/Images/1-s2.0-S1097276511007131-fx1_lrg.jpg'\n",
    "annotations = process_image(image_path)\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/wyfrzx6n449_pdz7fclsbkjh0000gn/T/ipykernel_6895/2666900052.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['processed_image'] = df_test['saved_image_path'].apply(process_image)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>saved_image_path</th>\n",
       "      <th>processed_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tumor immune surveillance and cancer immunothe...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "      <td>[{'text': 'Ecdysone', 'coordinates': [165, 87,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MicroRNAs (miRNAs) regulate gene expression th...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "      <td>[{'text': 'IL-1O', 'coordinates': [1080, 0, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The mechanisms by which ubiquitin ligases are ...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "      <td>[{'text': 'AUG', 'coordinates': [529, 180, 626...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The maintenance of H3K9 and DNA methylation at...</td>\n",
       "      <td>/Users/stevensu/Desktop/SciSketch-Summer/Image...</td>\n",
       "      <td>[{'text': 'A', 'coordinates': [0, 0, 55, 59]},...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  Tumor immune surveillance and cancer immunothe...   \n",
       "1  MicroRNAs (miRNAs) regulate gene expression th...   \n",
       "2  The mechanisms by which ubiquitin ligases are ...   \n",
       "3  The maintenance of H3K9 and DNA methylation at...   \n",
       "\n",
       "                                    saved_image_path  \\\n",
       "0  /Users/stevensu/Desktop/SciSketch-Summer/Image...   \n",
       "1  /Users/stevensu/Desktop/SciSketch-Summer/Image...   \n",
       "2  /Users/stevensu/Desktop/SciSketch-Summer/Image...   \n",
       "3  /Users/stevensu/Desktop/SciSketch-Summer/Image...   \n",
       "\n",
       "                                     processed_image  \n",
       "0  [{'text': 'Ecdysone', 'coordinates': [165, 87,...  \n",
       "1  [{'text': 'IL-1O', 'coordinates': [1080, 0, 13...  \n",
       "2  [{'text': 'AUG', 'coordinates': [529, 180, 626...  \n",
       "3  [{'text': 'A', 'coordinates': [0, 0, 55, 59]},...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.head(4)\n",
    "\n",
    "df_test['processed_image'] = df_test['saved_image_path'].apply(process_image)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Crosslink RNA to protein', 'coordinates': [182, 32, 393, 109]}, {'text': '228', 'coordinates': [792, 38, 848, 70]}, {'text': 'eCLIPs', 'coordinates': [858, 37, 964, 69]}, {'text': 'CDS', 'coordinates': [688, 92, 740, 122]}, {'text': \"5' UTR\", 'coordinates': [580, 134, 670, 164]}, {'text': 'Immunoprecipitate RNA-protein complexes', 'coordinates': [182, 140, 521, 217]}, {'text': 'RNA', 'coordinates': [702, 174, 754, 202]}, {'text': 'Splice', 'coordinates': [798, 190, 892, 222]}, {'text': 'site', 'coordinates': [900, 190, 964, 218]}, {'text': 'Sequence and align reads', 'coordinates': [182, 244, 393, 323]}, {'text': 'tRNA/ snRNA', 'coordinates': [618, 264, 770, 294]}, {'text': 'SnoRNA', 'coordinates': [508, 280, 598, 306]}, {'text': 'MtRNA', 'coordinates': [732, 302, 814, 332]}, {'text': \"3' UTR\", 'coordinates': [876, 310, 966, 340]}, {'text': 'Call RNA elements enriched over input', 'coordinates': [183, 353, 459, 431]}, {'text': 'Intron', 'coordinates': [796, 356, 890, 382]}, {'text': 'Evaluate performance of binding site detection', 'coordinates': [174, 452, 824, 501]}, {'text': 'GUUCAAACUCAUGG GAGUGUGACCUAAU AAUGUUCAGGCUUU', 'coordinates': [604, 510, 814, 594]}, {'text': '=', 'coordinates': [884, 512, 972, 584]}, {'text': 'Discover constrained binding via in silico perturbation reference common rare very rare singleton', 'coordinates': [127, 635, 869, 809]}, {'text': 'binding binding', 'coordinates': [71, 689, 184, 790]}, {'text': 'AGTGCCTGTGGTTGTCAGA', 'coordinates': [638, 678, 922, 708]}, {'text': 'GT G C C TC C A A C A A CCAG G A', 'coordinates': [638, 706, 921, 808]}, {'text': 'Nominate new splicing regulators', 'coordinates': [266, 830, 735, 880]}]\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType\n",
    "import os\n",
    "\n",
    "# Define the function to process images\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=70, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "\n",
    "    return text_annotations\n",
    "\n",
    "# # Define the UDF to be used in PySpark\n",
    "# @udf(returnType=ArrayType(StructType([\n",
    "#     StructField(\"text\", StringType(), True),\n",
    "#     StructField(\"coordinates\", ArrayType(IntegerType()), True)\n",
    "# ])))\n",
    "\n",
    "# def process_image_udf(image_path):\n",
    "#     return process_image(image_path)\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"ImageProcessingApp\").getOrCreate()\n",
    "\n",
    "# # Define schema for DataFrame\n",
    "# schema = StructType([\n",
    "#     StructField(\"Abstract\", StringType(), True),\n",
    "#     StructField(\"saved_image_path\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # Convert the processed Pandas DataFrame to a Spark DataFrame\n",
    "# sdf = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "# # Repartition the DataFrame\n",
    "# sdf = sdf.repartition(40)  # Adjust the number based on your system's capacity\n",
    "\n",
    "# # Apply the UDF to process images\n",
    "# sdf = sdf.withColumn(\"text_annotations\", process_image_udf(sdf[\"saved_image_path\"]))\n",
    "\n",
    "# # Select the relevant columns\n",
    "# sdf = sdf.select(\"Abstract\", \"text_annotations\")\n",
    "\n",
    "# # Convert to JSON format and save the output\n",
    "# sdf.write.json(\"/Users/stevensu/Desktop/SciSketch-Dataset/processed_data.json\")\n",
    "\n",
    "# # Stop Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:167\u001b[0m, in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image_path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load the main image using OpenCV\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/__init__.py:157\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Set a global controller that can be used to locally limit the number of\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# threads without looping through all shared libraries every time.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# This instantitation should not happen earlier because it needs all BLAS and\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# OpenMP libs to be loaded first.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     _threadpool_controller \u001b[38;5;241m=\u001b[39m \u001b[43mThreadpoolController\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_module\u001b[39m(module):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fixture for the tests to assure globally controllable seeding of RNGs\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:818\u001b[0m, in \u001b[0;36mThreadpoolController.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib_controllers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:970\u001b[0m, in \u001b[0;36mThreadpoolController._load_libraries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded shared libraries and store the supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_libraries_with_dyld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_libraries_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:1040\u001b[0m, in \u001b[0;36mThreadpoolController._find_libraries_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1037\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# Store the library controller if it is supported and selected\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_controller_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:1134\u001b[0m, in \u001b[0;36mThreadpoolController._make_controller_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Store a library controller if it is supported and selected\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# Required to resolve symlinks\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[43m_realpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# `lower` required to take account of OpenMP dll case on Windows\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# (vcomp, VCOMP, Vcomp, ...)\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filepath)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:548\u001b[0m, in \u001b[0;36m_realpath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_realpath\u001b[39m(filepath):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Small caching wrapper around os.path.realpath to limit system calls\"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:391\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 391\u001b[0m     path, ok \u001b[38;5;241m=\u001b[39m \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:425\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, seen)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    424\u001b[0m newpath \u001b[38;5;241m=\u001b[39m join(path, name)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mislink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    426\u001b[0m     path \u001b[38;5;241m=\u001b[39m newpath\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/posixpath.py:167\u001b[0m, in \u001b[0;36mislink\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a symbolic link\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Load the main image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from path {image_path}\")\n",
    "        return None\n",
    "\n",
    "    original_image = image.copy()\n",
    "\n",
    "    # Initialize the EasyOCR reader with GPU support if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        reader = easyocr.Reader(['en'], gpu=True)  # Use MPS (Metal Performance Shaders) backend\n",
    "    else:\n",
    "        reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    results = reader.readtext(original_image)\n",
    "\n",
    "    # Extract the bounding box coordinates and text\n",
    "    boxes = [item[0] for item in results]\n",
    "    texts = [item[1] for item in results]\n",
    "\n",
    "    # Calculate the center of each bounding box\n",
    "    centers = np.array([[(box[0][0] + box[2][0]) / 2, (box[0][1] + box[2][1]) / 2] for box in boxes])\n",
    "\n",
    "    # Cluster the bounding boxes using DBSCAN\n",
    "    clustering = DBSCAN(eps=50, min_samples=1).fit(centers)\n",
    "\n",
    "    # Group the boxes and texts by cluster\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = {\n",
    "                'boxes': [],\n",
    "                'texts': []\n",
    "            }\n",
    "        clusters[label]['boxes'].append(boxes[idx])\n",
    "        clusters[label]['texts'].append(texts[idx])\n",
    "\n",
    "    text_annotations = []\n",
    "    for cluster in clusters.values():\n",
    "        combined_text = ' '.join(cluster['texts'])\n",
    "\n",
    "        # Get the bounding box of the cluster\n",
    "        x_min = min([box[0][0] for box in cluster['boxes']])\n",
    "        y_min = min([box[0][1] for box in cluster['boxes']])\n",
    "        x_max = max([box[2][0] for box in cluster['boxes']])\n",
    "        y_max = max([box[2][1] for box in cluster['boxes']])\n",
    "\n",
    "        # Convert to integers\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Append to the list of text annotations\n",
    "        text_annotations.append({\n",
    "            'text': combined_text,\n",
    "            'coordinates': [x_min, y_min, x_max, y_max]\n",
    "        })\n",
    "\n",
    "    return text_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 154/24683 [07:52<20:54:35,  3.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             training_data\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n\u001b[0;32m---> 32\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     35\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(training_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mcreate_training_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m training_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         training_data\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_image_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process the image and get text annotations\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m text_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Skip if the image could not be processed\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize the EasyOCR reader with GPU support if available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 20\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use MPS (Metal Performance Shaders) backend\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m], gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/easyocr/easyocr.py:231\u001b[0m, in \u001b[0;36mReader.__init__\u001b[0;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     network_params \u001b[38;5;241m=\u001b[39m recog_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m \u001b[43mget_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecog_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdict_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/easyocr/recognition.py:182\u001b[0m, in \u001b[0;36mget_recognizer\u001b[0;34m(recog_network, network_params, character, separator_list, dict_list, model_path, device, quantize)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(model)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 182\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, converter\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:1313\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/serialization.py:307\u001b[0m, in \u001b[0;36m_mps_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mps_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SciSketch-Dataset/.venv/lib/python3.12/site-packages/torch/storage.py:144\u001b[0m, in \u001b[0;36m_StorageBase.mps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a MPS copy of this storage if it's not already on the MPS.\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# def process_row(row):\n",
    "#     abstract_text = row['Abstract']\n",
    "#     image_path = row['saved_image_path']\n",
    "\n",
    "#     # Process the image and get text annotations\n",
    "#     text_annotations = process_image(image_path)\n",
    "\n",
    "#     # Skip if the image could not be processed\n",
    "#     if text_annotations is None:\n",
    "#         return None\n",
    "\n",
    "#     # Create the structured data entry\n",
    "#     data_entry = {\n",
    "#         'abstract_text': abstract_text,\n",
    "#         'text_annotations': text_annotations\n",
    "#     }\n",
    "\n",
    "#     return data_entry\n",
    "\n",
    "# def create_training_data(df):\n",
    "#     training_data = []\n",
    "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "#         result = process_row(row)\n",
    "#         if result is not None:\n",
    "#             training_data.append(result)\n",
    "    \n",
    "#     return training_data\n",
    "\n",
    "# training_data = create_training_data(df)\n",
    "\n",
    "# with open('training_data.json', 'w') as f:\n",
    "#     json.dump(training_data, f, indent=4)\n",
    "\n",
    "# # Print the structured training data\n",
    "# for entry in training_data:\n",
    "#     print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_data_for_bert(training_data):\n",
    "    inputs = []\n",
    "    text_labels = []\n",
    "    coord_labels = []\n",
    "\n",
    "    for entry in training_data:\n",
    "        abstract_text = entry['abstract_text']\n",
    "        text_annotations = entry['text_annotations']\n",
    "\n",
    "        # Tokenize the abstract text\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            abstract_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Create labels for text presence and coordinates\n",
    "        text_label = torch.zeros(input_ids.size())\n",
    "        coord_label = torch.zeros((input_ids.size(0), 4))\n",
    "\n",
    "        for annotation in text_annotations:\n",
    "            text = annotation['text']\n",
    "            coordinates = annotation['coordinates']\n",
    "\n",
    "            # Find the start and end tokens of the text in the abstract\n",
    "            start_idx = abstract_text.find(text)\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(text) - 1\n",
    "\n",
    "                # Convert character indices to token indices\n",
    "                token_start_idx = tokenizer.encode(abstract_text[:start_idx], add_special_tokens=False)\n",
    "                token_end_idx = tokenizer.encode(abstract_text[:end_idx + 1], add_special_tokens=False)\n",
    "\n",
    "                token_start_idx = len(token_start_idx)\n",
    "                token_end_idx = len(token_end_idx)\n",
    "\n",
    "                text_label[token_start_idx:token_end_idx] = 1  # Mark the presence of text\n",
    "                coord_label[token_start_idx:token_end_idx, :] = torch.tensor(coordinates)  # Assign coordinates\n",
    "\n",
    "        inputs.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        })\n",
    "        text_labels.append(text_label)\n",
    "        coord_labels.append(coord_label)\n",
    "\n",
    "    return inputs, text_labels, coord_labels\n",
    "\n",
    "inputs, text_labels, coord_labels = prepare_data_for_bert(training_data)\n",
    "\n",
    "# Example of how the inputs and labels look\n",
    "print(inputs[0])\n",
    "print(text_labels[0])\n",
    "print(coord_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:   5%|▍         | 112/2437 [00:04<02:10, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/virus.svg: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  15%|█▌        | 377/2437 [00:18<00:53, 38.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_spiral.svg: <urlopen error [Errno 2] No such file or directory: '/Users/stevensu/Desktop/BioIcons All Images/7444463B.jpg'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  88%|████████▊ | 2141/2437 [01:41<00:09, 31.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Polyomaviridae.svg: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  90%|████████▉ | 2191/2437 [01:44<00:11, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_coccus.svg: <urlopen error [Errno 2] No such file or directory: '/Users/stevensu/Desktop/BioIcons All Images/DA34F080.jpg'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG:  97%|█████████▋| 2354/2437 [01:54<00:03, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting /Users/stevensu/Desktop/BioIcons All Images/Anterior_view_of_ligaments_of_pelvis.svg: junk after document element: line 3993, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting SVG to PNG: 100%|██████████| 2437/2437 [01:58<00:00, 20.49it/s]\n",
      "Template Matching: 100%|██████████| 6381/6381 [00:00<00:00, 27773.16it/s]\n",
      "[ WARN:10@2670.354] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/virus_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:4@2672.677] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Anterior_view_of_ligaments_of_pelvis_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:4@2697.742] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Polyomaviridae_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:9@2720.353] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_coccus_converted.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:6@2740.402] global loadsave.cpp:248 findDecoder imread_('/Users/stevensu/Desktop/BioIcons All Images/Bacteria2M_spiral_converted.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import cairosvg\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "# from dask import delayed, compute\n",
    "\n",
    "# def convert_svg_to_png(svg_path, output_path):\n",
    "#     try:\n",
    "#         cairosvg.svg2png(url=svg_path, write_to=output_path, unsafe=True)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error converting {svg_path}: {e}\")\n",
    "\n",
    "# # Load the main image using OpenCV\n",
    "# image_path = '/Users/stevensu/Desktop/SciSketch-Dataset/Images/1-s2.0-S2667290121000632-gr1_lrg.jpg'  # Update with the correct path if needed\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Check if the image was loaded successfully\n",
    "# if image is None:\n",
    "#     raise FileNotFoundError(f\"Cannot open image file: {image_path}\")\n",
    "\n",
    "# original_image = image.copy()\n",
    "\n",
    "# # List of template image paths\n",
    "# png_template_paths = glob.glob('/Users/stevensu/Desktop/Servier Medical Art All Images/*.png')\n",
    "# svg_template_paths = glob.glob('/Users/stevensu/Desktop/BioIcons All Images/*.svg')\n",
    "\n",
    "# # Convert SVG templates to PNG\n",
    "# for svg_path in tqdm(svg_template_paths, desc=\"Converting SVG to PNG\"):\n",
    "#     output_path = svg_path.replace('.svg', '_converted.png')\n",
    "#     convert_svg_to_png(svg_path, output_path)\n",
    "#     png_template_paths.append(output_path)\n",
    "\n",
    "# @delayed\n",
    "# def match_template(template_path):\n",
    "#     template = cv2.imread(template_path, 0)\n",
    "#     if template is None:\n",
    "#         return []\n",
    "\n",
    "#     if template.shape[0] > original_image.shape[0] or template.shape[1] > original_image.shape[1]:\n",
    "#         return []\n",
    "\n",
    "#     w, h = template.shape[::-1]\n",
    "#     res = cv2.matchTemplate(cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY), template, cv2.TM_CCOEFF_NORMED)\n",
    "#     threshold = 0.8  # Adjust this threshold as needed\n",
    "#     loc = np.where(res >= threshold)\n",
    "    \n",
    "#     boxes = []\n",
    "#     for pt in zip(*loc[::-1]):\n",
    "#         box = [pt, (pt[0] + w, pt[1] + h)]\n",
    "#         boxes.append(box)\n",
    "#         cv2.rectangle(image, pt, (pt[0] + w, pt[1] + h), (255, 0, 0), 2)\n",
    "    \n",
    "#     return boxes\n",
    "\n",
    "# # Template matching with progress bar\n",
    "# tasks = []\n",
    "# for template_path in tqdm(png_template_paths, desc=\"Template Matching\"):\n",
    "#     result = match_template(template_path)\n",
    "#     tasks.append(result)\n",
    "\n",
    "# # Compute the results in parallel\n",
    "# computed_results = compute(*tasks)\n",
    "\n",
    "# # Collect all detected icon locations\n",
    "# icon_locations = []\n",
    "# for loc in computed_results:\n",
    "#     if loc:\n",
    "#         icon_locations.extend(loc)\n",
    "\n",
    "# # Draw bounding boxes for detected icons\n",
    "# for loc in icon_locations:\n",
    "#     cv2.rectangle(image, loc[0], loc[1], (255, 0, 0), 2)\n",
    "\n",
    "# # Display the image with bounding boxes\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
